% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kappa-3-table.R
\name{kappa_3_table}
\alias{kappa_3_table}
\title{Fleiss' generalized kappa coefficient}
\usage{
kappa_3_table(ratings, weights = "unweighted", conf_lev = 0.95,
  N = Inf)
}
\arguments{
\item{ratings}{This is an n×q matrix (or contingency table), where n is
the number of subjects, and q the number of categories. This is the only
argument that must be specified if you want an unweighted analysis.#'}

\item{weights}{One of the following to calculate weight based on defined
methods: "unweighted", "quadratic", "linear", "ordinal", "radical",
"ratio", "circular", "bipolar". The default is "unweighted", a diagonal
matrix where all diagonal numbers equal to 1, and all off-diagonal numbers
equal to 0. This special weight matrix leads to the unweighted analysis.
You may specify your own q × q weight matrix here}

\item{conf_lev}{The confidence level associated with the agreement
coefficient’s confidence interval. Default is 0.95.}

\item{N}{An optional parameter representing the total number of subjects in
the target subject population. Its default value is infinity, which for all
practical purposes assumes the target subject population to be very large
and will not require any finite-population correction when computing the
standard error.}
}
\value{
A tbl_df with the coefficient, standard error, lower and upper confidence
limits.
}
\description{
Fleiss' generalized kappa coefficient (see Fleiss(1971)) and its standard
error for 3 raters or more when input dataset is a n×q matrix representing
the distribution of raters by subject and by category.

Ratings are represented as a table where each row represents one subject,
each column represents one category, and each table cell represents the
number of raters who classified the specified subject into the specified
category.

A table cell may have a 0 value if none of the raters classified the subject
into the category associated with that cell. The number of raters may vary by
subject leading to a table with different row totals. That will be the case
when the experiment generated missing ratings, with subjects being rated by a
different number of raters.

Exclude all subjects that are not rated by any rater.
}
\examples{

library(dplyr)

#### Example 1 --------------------------------
# Distribution of 6 Raters by Subject and Category (Extract of Table 1 of
# Fleiss, 1971)
ratings <- matrix(c(0, 0, 0, 6, 0,
                    0, 1, 4, 0, 1,
                    2, 0, 4, 0, 0,
                    0, 3, 3, 0, 0),
                  ncol = 5,
                  byrow = TRUE)

kappa_3_table(ratings = ratings,
              weights = "unweighted")

kappa_3_table(ratings = ratings,
              weights = "quadratic")

#### Example 2 --------------------------------
# Rating of Four Subjects by Five Raters (Table 2 of Finn, 1970)
finn_1970_table <- tibble::tribble(
  ~subject, ~rater_I, ~rater_II, ~rater_III, ~rater_IV, ~rater_V,
  "A",        2,         2,          3,         2,        2,
  "B",        2,         2,          2,         2,        2,
  "C",        2,         2,          2,         2,        1,
  "D",        1,         2,          2,         2,        2
) \%>\%
  mutate_at(.vars = vars(dplyr::starts_with("rater")),
            .funs = list(~ factor(.,
                                  levels = c(1, 2, 3))))

calc_agree_mat(data = finn_1970_table,
               dplyr::starts_with("rater"),
               subject_id = "subject") \%>\%
  kappa_3_table(ratings = .)

calc_agree_mat(data = finn_1970_table,
               dplyr::starts_with("rater"),
               subject_id = "subject") \%>\%
  kappa_3_table(ratings = .,
                weights = "quadratic")
}
\references{
Fleiss, J. L. (1981). Statistical Methods for Rates and Proportions. John
Wiley & Sons.
\enumerate{
\item Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring
the Extent of Agreement Among Raters. 4th ed. Gaithersburg, MD: Advanced
Analytics.
}
}
