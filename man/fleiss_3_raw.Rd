% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ac-fleiss-kappa.R
\name{fleiss_3_raw}
\alias{fleiss_3_raw}
\alias{scott_2_table}
\alias{fleiss_3_dist}
\title{Fleiss' generalized kappa coefficient among multiple raters (2, 3, +)}
\usage{
fleiss_3_raw(
  data,
  ...,
  weights = "unweighted",
  categ = NULL,
  conf_lev = 0.95,
  N = Inf,
  test_value = 0,
  alternative = "two.sided"
)

scott_2_table(
  table,
  weights = "unweighted",
  conf_lev = 0.95,
  N = Inf,
  test_value = 0,
  alternative = "two.sided"
)

fleiss_3_dist(
  distribution,
  weights = "unweighted",
  categ = NULL,
  conf_lev = 0.95,
  N = Inf,
  test_value = 0,
  alternative = "two.sided"
)
}
\arguments{
\item{data}{A data frame or tibble}

\item{...}{Variable (column) names containing the ratings where each column
represents one rater and each row one subject.}

\item{weights}{is an optional parameter that is either a string variable or a
matrix. The string describes one of the predefined weights and must take
one of the values ("quadratic", "ordinal", "linear", "radical", "ratio",
"circular", "bipolar"). If this parameter is a matrix then it must be a
square matrix qxq where q is the number of possible categories where a
subject can be classified. If some of the q possible categories are not
used, then it is strongly advised to specify the complete list of possible
categories as a vector in parameter \code{categ}. Otherwise, only the categories
reported will be used.}

\item{categ}{An optional parameter representing all categories available to
raters during the experiment. This parameter may be useful if some
categories were not used by any rater in spite of being available to the
raters.}

\item{conf_lev}{The confidence level associated with the agreement
coefficient’s confidence interval. Default is 0.95.}

\item{N}{An optional parameter representing the total number of subjects in
the target subject population. Its default value is infinity, which for all
practical purposes assumes the target subject population to be very large
and will not require any finite-population correction when computing the
standard error.}

\item{test_value}{value to test the estimated AC against. Default is 0.}

\item{alternative}{a character string specifying the alternative hypothesis,
must be one of "two.sided" (default), "greater" or "less".}

\item{table}{A q×q matrix (or contingency table) showing the distribution
of subjects by rater, where q is the number of categories. This is
the only argument you must specify if you want the unweighted analysis}

\item{distribution}{An \emph{nxq} matrix / data frame containing the distribution
of raters by subject and category. Each cell \emph{(i,k)} contains the
number of raters who classified subject \emph{i} into category \emph{k}. An n
x q agreement matrix representing the distribution of raters by subjects (n)
and category (q) (see \code{calc_agree_mat} to convert raw data to distribution).}
}
\value{
A tbl_df with the coefficient, standard error, lower and upper confidence
limits.

A tbl_df with the coefficient, standard error, lower and upper confidence
limits.

A tbl_df with the coefficient, standard error, lower and upper confidence
limits.
}
\description{
Fleiss' generalized kappa among multiple raters (2, 3, +) when the input data
represent the raw ratings reported for each subject and each rater.

Scott's pi coefficient (Scott(1955)) and its standard error for 2
raters when input dataset is a contingency table.

This function computes Fleiss' generalized kappa coefficient (see
Fleiss(1971)) and its standard error for 3 raters or more when input dataset
is a nxq matrix representing the distribution of raters by subject and by
category.
}
\examples{
#  5 raters classify 10 subjects into 1 of 3 rating categories
rvary2

# More than two raters
fleiss_3_raw(data = rvary2,
             dplyr::starts_with("rater"))

# Two raters
fleiss_3_raw(data = rvary2,
             rater1:rater2)

# Another example with two raters
# two radiologists who classify 85 xeromammograms into one of four categories
# (Altman p. 403)
radiologist

fleiss_3_raw(data = radiologist,
             radiologist_a, radiologist_b)
ratings <- matrix(c(5, 3, 0, 0,
                    3, 11, 4, 0,
                    2, 13, 3, 4,
                    1, 2, 4, 14), ncol = 4, byrow = TRUE)

scott_2_table(table = ratings)

scott_2_table(table = ratings,
              weights = "quadratic")

scott_2_table(table = ratings,
              weights = ac_weights(categ = c(1:4),
                                   weight_type = "quadratic"))

my_weights <- matrix(c(1.0000000, 0.8888889, 0.5555556, 0.0000000,
                       0.8888889, 1.0000000, 0.8888889, 0.5555556,
                       0.5555556, 0.8888889, 1.0000000, 0.8888889,
                       0.0000000, 0.5555556, 0.8888889, 1.0000000),
                     ncol = 4, byrow = TRUE)

scott_2_table(table = ratings,
              weights = my_weights)
library(tidyverse)

rvary2 <- tibble::tribble(
            ~subject, ~rater1, ~rater2, ~rater3, ~rater4, ~rater5,
                  1L,      1L,      2L,      2L,      NA,      2L,
                  2L,      1L,      1L,      3L,      3L,      3L,
                  3L,      3L,      3L,      3L,      3L,      3L,
                  4L,      1L,      1L,      1L,      1L,      3L,
                  5L,      1L,      1L,      1L,      3L,      3L,
                  6L,      1L,      2L,      2L,      2L,      2L,
                  7L,      1L,      1L,      1L,      1L,      1L,
                  8L,      2L,      2L,      2L,      2L,      3L,
                  9L,      1L,      3L,      NA,      NA,      3L,
                 10L,      1L,      1L,      1L,      3L,      3L
            )

ex_dist <- calc_agree_mat(data = rvary2,
                          dplyr::starts_with("rater"),
                          subject_id = subject)

ex_dist

fleiss_3_dist(distribution = ex_dist)
}
\references{
\enumerate{
\item Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring
the Extent of Agreement Among Raters. 4th ed. Gaithersburg, MD: Advanced
Analytics.
}

Fleiss, J. L. (1981). Statistical Methods for Rates and Proportions. John
Wiley & Sons.

Scott (1955)
\enumerate{
\item Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring
the Extent of Agreement Among Raters. 4th ed. Gaithersburg, MD: Advanced
Analytics.
}

\enumerate{
\item Handbook of Inter-Rater Reliability: The Definitive Guide to Measuring
the Extent of Agreement Among Raters. 4th ed. Gaithersburg, MD: Advanced
Analytics.
}

Fleiss, J. L. (1981). Statistical Methods for Rates and Proportions. John
Wiley & Sons.
}
